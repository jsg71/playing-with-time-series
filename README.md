
# Lightning Burst Detection with Deep Learning & Compression

## Introduction & Motivation

This project provides a complete pipeline for detecting **lightning bursts** (short, highâ€‘intensity events in a timeâ€‘series signal) using both deepâ€‘learning models and compressionâ€‘based algorithms. The goal is to identify when a lightning strike (or similar burst event) occurs within a noisy continuous signal.

We explore three complementary approaches:

| Approach | Learning Type | Key Script(s) | Training Needed? |
|----------|---------------|---------------|------------------|
| **NCD** (Normalised Compression Distance) | Heuristic / unsupervised | `run_ncd.py` | **No** |
| **Autoencoder (AE)** | Unsupervised deep learning | `train_ae.py`,Â `eval_ae.py`  | YesÂ â€” trains only on noise |
| **RawResNet1D** | Supervised deep learning | `train_resnet.py`,Â `eval_resnet.py` | YesÂ â€” needs labelled bursts |

A syntheticâ€‘data simulator is included so you can benchmark everything endâ€‘toâ€‘end without hunting for real lightning recordings.

---

## Project Layout

```
â”œâ”€â”€ scripts/              # Commandâ€‘line entryâ€‘points
â”‚   â”œâ”€â”€ sim_make.py
â”‚   â”œâ”€â”€ train_ae.py
â”‚   â”œâ”€â”€ train_ae_baseline.py
â”‚   â”œâ”€â”€ eval_ae.py
â”‚   â”œâ”€â”€ eval_ae_baseline.py
â”‚   â”œâ”€â”€ run_ncd.py
â”‚   â”œâ”€â”€ train_resnet.py
â”‚   â””â”€â”€ eval_resnet.py
â”œâ”€â”€ leela_ml/             # Core library code
â”‚   â”œâ”€â”€ signal_sim/       # Synthetic signal generator
â”‚   â”œâ”€â”€ datamodules_npy.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ dae_unet.py
â”‚       â”œâ”€â”€ raw_resnet.py
â”‚       â””â”€â”€ ncd.py
â”œâ”€â”€ configs/              # YAML configs for Lightning scripts
â”œâ”€â”€ data/                 # Generated data lives here
â”œâ”€â”€ reports/              # Plots & metrics
â””â”€â”€ requirements.txt
```

---

## 1Â Â EnvironmentÂ (setâ€‘up once)

```bash
python -m venv .venv
source .venv/bin/activate          # Linux / macOS
python -m pip install -U pip
pip install -r requirements.txt    # installs PyTorchÂ + Lightning + utils
```

*Linux + NVIDIA GPU* gives the best speed; AppleÂ Mâ€‘series works via `--device mps`.  
CPUâ€‘only machines are perfectly fine for small demos (just slower).

---

## 2Â Â Generate Synthetic Data

```bash
python scripts/sim_make.py   --minutes 5   --out data/synthetic/storm5   --seed 42
```

| Flag        | Meaning | Default |
|-------------|---------|---------|
| `--minutes` | Realâ€‘time length of recording to simulate | `5` |
| `--out`     | Prefix for output files | **required** |
| `--seed`    | RNG seed for full reproducibility | `0` |

Creates:

```
data/synthetic/
 â”œâ”€ storm5_0.npy        # raw waveform (float32)
 â”œâ”€ storm5_meta.json    # {"fs": 40000, "events":[â€¦]}
 â””â”€ storm5_wave.npy     # alias copy of first channel
```

---

## 3Â Â Unsupervised Pipelines

### 3.1Â Â Train Autoencoder (`train_ae.py`)

```bash
python scripts/train_ae.py   --npy   data/synthetic/storm5_wave.npy   --meta  data/synthetic/storm5_meta.json   --chunk 4096 --overlap 0.5   --bs 128 --epochs 20   --depth 4 --base 16   --ckpt lightning_logs/ae_best.ckpt
```

Important parameters:

| Flag | What it controls | Typical values |
|------|------------------|----------------|
| `--chunk` | Window length fed to AE | 2048Â â€“Â 8192 |
| `--overlap` | Fractional overlap between windows | 0.3Â â€“Â 0.8 |
| `--depth` / `--base` | Uâ€‘Net capacity | deeper = larger RF |
| `--noise_std` | Extra Gaussian noise during training | 0Â â€“Â 0.2 |

Output: best checkpoint + `.split.npz` with train/val/test indices.

### 3.2Â Â Detect Bursts with AE (`eval_ae.py`)

```bash
python scripts/eval_ae.py   --npy data/synthetic/storm5_wave.npy   --meta data/synthetic/storm5_meta.json   --ckpt lightning_logs/ae_best.ckpt   --chunk 512 --overlap 0.9   --mad_k 6 --win_ms 100 --fig_dark
```

Plots appear in `reports/` and metrics in console.  
Tune `--mad_k` to trade PrecisionÂ â†”Â Recall.

### 3.3Â Â Compression Detector (`run_ncd.py`)

```bash
python scripts/run_ncd.py   --npy data/synthetic/storm5_wave.npy   --meta data/synthetic/storm5_meta.json   --chunk 512 --overlap 0.9   --codec zlib --mad_k 6
```

No training needed â€”Â baseline that works everywhere.

---

## 4Â Â Supervised Pipeline

### 4.1Â Â Train ResNet (`train_resnet.py`)

```bash
python scripts/train_resnet.py   --npy data/synthetic/storm5_wave.npy   --meta data/synthetic/storm5_meta.json   --chunk 8192 --overlap 0.75   --bs 64 --epochs 40   --ckpt lightning_logs/raw_best.ckpt
```

Eventâ€‘aware `GroupShuffleSplit` keeps every lightning event in exactly one split â†’ no leakage.

### 4.2Â Â Evaluate ResNet (`eval_resnet.py`)

```bash
python scripts/eval_resnet.py   --npy  data/synthetic/storm5_wave.npy   --meta data/synthetic/storm5_meta.json   --chunk 8192   --ckpt lightning_logs/raw_best.ckpt   --bs 512
```

Outputs AUROC + F1 for **val** & **test**, plus a dualâ€‘panel plot.

---

## 5Â Â Method ComparisonÂ (on synthetic, default params)

| Detector | Training need | EventÂ F1 (â‰ˆ) | Strengths | Weaknesses |
|----------|---------------|--------------|-----------|------------|
| **NCD**  | none          | 0.65 | zero setup | sensitive to any change |
| **AE**   | noise only    | 0.87 | unsupervised, adaptive | needs threshold tuning |
| **ResNet** | labelled bursts | 0.95+ | highest accuracy | needs labels, training |

---

## 6Â Â Troubleshooting

| Symptom | Fix |
|---------|-----|
| *CUDA device not found* | `pip install torch==x.y.z+cpu -f https://download.pytorch.org/whl/torch_stable.html` |
| AE flags too many FP | increase `--mad_k` or retrain with lower `--noise_std` |
| ResNet overfits | larger `--chunk`, add dropout, or more training data |

---

## 7Â Â Contributing

1. Run `ruff` and `black` before committing.  
2. Add/adjust unit tests in `tests/`.  
3. Open an issue for big changes first.

---

### Future Ideas

* Multiâ€‘channel fusion, streaming inference, zstd codec for NCD, hyperâ€‘param sweeps with Optuna, explainable Gradâ€‘CAM for ResNet, etc.
 ðŸš€

