{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Detection with NCD\n",
    "This notebook demonstrates compression-based lightning detection using **Normalised Compression Distance** (NCD). We also compare a simple amplitude-threshold baseline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from pandas import Series\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from leela_ml.datamodules_npy import StrikeDataset\n",
    "from leela_ml.ncd import ncd_adjacent, ncd_first\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from leela_ml.signal_sim.simulator import simulate\n",
    "out_prefix = Path('data/demo')\n",
    "simulate(1, str(out_prefix), seed=0)\n",
    "npy = 'data/demo_LON.npy'\n",
    "meta = 'data/demo_meta.json'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds = StrikeDataset(npy, meta, chunk_size=512, overlap=0.9)\n",
    "win = ds._windows.astype(np.float32, copy=False)\n",
    "lab = ds.labels.astype(bool)\n",
    "fs = ds.fs; hop = ds.hop\n",
    "print(\"windows\", ds.n_win, \"positives\", int(lab.sum()))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NCD computation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "err = ncd_adjacent(win, per_win_norm=True)\n",
    "win_len = max(1, int(0.01 * fs / hop))\n",
    "thr = Series(err).rolling(win_len, center=True, min_periods=1).median() + 6*Series(err).rolling(win_len, center=True, min_periods=1).apply(lambda v: np.median(np.abs(v-np.median(v))), raw=True)\n",
    "mask = err > thr.values\n",
    "tn, fp, fn, tp = confusion_matrix(lab, mask).ravel()\n",
    "P,R,F,_ = precision_recall_fscore_support(lab, mask, average='binary')\n",
    "metrics_ncd = dict(P=float(P), R=float(R), F1=float(F), TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "err_first = ncd_first(win, baseline_idx=0, per_win_norm=True)\n",
    "thr_first = Series(err_first).rolling(win_len, center=True, min_periods=1).median() + 6*Series(err_first).rolling(win_len, center=True, min_periods=1).apply(lambda v: np.median(np.abs(v-np.median(v))), raw=True)\n",
    "mask_first = err_first > thr_first.values\n",
    "tn, fp, fn, tp = confusion_matrix(lab, mask_first).ravel()\n",
    "P1,R1,F1,_ = precision_recall_fscore_support(lab, mask_first, average='binary')\n",
    "metrics_first = dict(P=float(P1), R=float(R1), F1=float(F1), TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple amplitude threshold baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "amp = np.sqrt((win**2).mean(axis=1))\n",
    "thr_amp = Series(amp).rolling(win_len, center=True, min_periods=1).median() + 6*Series(amp).rolling(win_len, center=True, min_periods=1).apply(lambda v: np.median(np.abs(v-np.median(v))), raw=True)\n",
    "mask_amp = amp > thr_amp.values\n",
    "tn, fp, fn, tp = confusion_matrix(lab, mask_amp).ravel()\n",
    "Pa,Ra,Fa,_ = precision_recall_fscore_support(lab, mask_amp, average='binary')\n",
    "metrics_amp = dict(P=float(Pa), R=float(Ra), F1=float(Fa), TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('NCD metrics', metrics_ncd)\n",
    "print('Amplitude metrics', metrics_amp)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot NCD and baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(err, label='NCD', lw=0.4)\n",
    "plt.plot(thr, '--', label='threshold', lw=0.8)\n",
    "plt.legend(); plt.title('NCD curve')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np, zlib, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import IsolationForest   # kept for comparison\n",
    "from scipy.signal import welch\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# ─── 1. simulate 60 s @100 kHz with 20 flashes ─────────────────────────────\n",
    "fs, dur, n_flashes = 100_000, 60, 20\n",
    "N = fs*dur\n",
    "flash_len = int(0.003*fs)                # 3 ms\n",
    "np.random.seed(42)\n",
    "signal = 0.2*np.random.randn(N).astype(np.float32)\n",
    "labels = np.zeros(N, bool)\n",
    "starts = np.sort(np.random.choice(N-flash_len, n_flashes, replace=False))\n",
    "for idx in starts:\n",
    "    t = np.arange(flash_len)/fs\n",
    "    signal[idx:idx+flash_len] += np.exp(-t/0.001)*np.cos(2*np.pi*4e3*t)\n",
    "    labels[idx:idx+flash_len] = True\n",
    "\n",
    "# ─── 2. windowing ───────────────────────────────────────────────────────────\n",
    "win, hop = 1024, 256                     # 75 % overlap\n",
    "n_win = (N-win)//hop + 1\n",
    "win_lab = np.array([labels[i*hop:i*hop+win].any() for i in range(n_win)])\n",
    "\n",
    "# ─── 3. STA / LTA ratio per window ──────────────────────────────────────────\n",
    "abs_sig = np.abs(signal)\n",
    "sta = np.convolve(abs_sig, np.ones(int(0.002*fs))/int(0.002*fs), mode='same')\n",
    "lta = np.convolve(abs_sig, np.ones(int(0.05*fs))/int(0.05*fs), mode='same') + 1e-6\n",
    "sta_lta = sta/lta\n",
    "# pick, for each window, the max STA/LTA inside that window\n",
    "ratio_win = np.array([sta_lta[i*hop:(i*hop+win)].max() for i in range(n_win)])\n",
    "\n",
    "# ─── 4. robust threshold (k·σ above mean) ───────────────────────────────────\n",
    "k = 6                                 # tuned once; still unsupervised\n",
    "thr = ratio_win.mean() + k*ratio_win.std()\n",
    "pred_win = ratio_win > thr               # boolean per window\n",
    "\n",
    "# ─── 5. window‑level metrics ───────────────────────────────────────────────\n",
    "P,R,F,_ = precision_recall_fscore_support(win_lab, pred_win, average='binary')\n",
    "tn,fp,fn,tp = confusion_matrix(win_lab, pred_win).ravel()\n",
    "window_metrics = dict(P=float(P), R=float(R), F1=float(F),\n",
    "                      TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n",
    "# → {'P': 0.92, 'R': 0.92, 'F1': 0.92,  TP=93, FP=8, FN=8, TN=23 325}\n",
    "\n",
    "# ─── 6. event‑level scoring (merge consecutive detections) ──────────────────\n",
    "def windows_to_events(flags):\n",
    "    events=[]; cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur is not None: events.append(tuple(cur)); cur=None\n",
    "    if cur is not None: events.append(tuple(cur))\n",
    "    return events\n",
    "\n",
    "det_evt  = windows_to_events(pred_win)\n",
    "true_evt = [(max(0,(idx-hop)//hop), min(n_win-1,(idx+flash_len)//hop))\n",
    "            for idx in starts]\n",
    "\n",
    "tp_e=sum(any(not(de<gs or ds>ge) for ds,de in det_evt) for gs,ge in true_evt)\n",
    "fn_e=len(true_evt)-tp_e\n",
    "fp_e=sum(not any(not(de<gs or ds>ge) for gs,ge in true_evt) for ds,de in det_evt)\n",
    "\n",
    "event_metrics = dict(P=tp_e/(tp_e+fp_e),\n",
    "                     R=tp_e/(tp_e+fn_e),\n",
    "                     F1=2*tp_e/max(1,tp_e*2+fp_e+fn_e),\n",
    "                     TP=tp_e, FP=fp_e, FN=fn_e)\n",
    "# → {'P': 0.91, 'R': 1.00, 'F1': 0.95, TP=20, FP=2, FN=0}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "event_metrics",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you don't already have them:\n",
    "# %pip install numpy pandas scikit-learn tqdm zstandard --quiet\n",
    "\n",
    "import json, zlib, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from leela_ml.signal_sim.simulator import simulate          # ← your code\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATA_ROOT   = Path(\"data/demo_run\")\n",
    "OUT_PREFIX  = DATA_ROOT / \"demo\"                             # ⇒ demo_LON.npy / demo_meta.json\n",
    "\n",
    "if not (OUT_PREFIX.with_name(OUT_PREFIX.name + \"_LON.npy\")).exists():\n",
    "    simulate(minutes=1, out_prefix=str(OUT_PREFIX), seed=42)\n",
    "else:\n",
    "    print(\"Using already‑generated files\")\n",
    "\n",
    "meta  = json.load(open(f\"{OUT_PREFIX}_meta.json\"))\n",
    "FS    = meta[\"fs\"]\n",
    "trace = np.load(f\"{OUT_PREFIX}_LON.npy\")                     # 1‑D float32 array\n",
    "print(f\"Loaded {trace.shape[0]/FS:,.1f} s of data @ {FS:,} Hz\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# derive sample‑level boolean array \"labels\" (True = lightning present)\n",
    "labels = np.zeros_like(trace, dtype=bool)\n",
    "\n",
    "for ev in meta[\"events\"]:\n",
    "    # find this station’s position in meta[\"stations\"]\n",
    "    st = next(s for s in meta[\"stations\"] if s[\"id\"] == \"LON\")\n",
    "    # same distance / delay calc as simulator\n",
    "    from math import radians, sin, cos, asin, sqrt\n",
    "    def hav_km(lat1, lon1, lat2, lon2):\n",
    "        R=6371\n",
    "        dlat, dlon = map(radians, (lat2-lat1, lon2-lon1))\n",
    "        a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "        return 2*R*asin(sqrt(a))\n",
    "    dist_km = hav_km(ev[\"lat\"], ev[\"lon\"], st[\"lat\"], st[\"lon\"])\n",
    "    delay   = dist_km / 3e5                           # C ≈ 3·10^5 km/s\n",
    "    i0      = int((ev[\"t\"] + delay) * FS)\n",
    "    dur     = int(0.04 * FS)                          # simulator uses 40 ms bursts\n",
    "    labels[i0 : i0+dur] = True\n",
    "\n",
    "n_pos = labels.sum()\n",
    "print(f\"Ground truth: {n_pos:,} positive samples \"\n",
    "      f\"({n_pos/len(labels)*100:.3f} %)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------- windowing ----------\n",
    "WIN, HOP = 1024, 256\n",
    "n_win    = (len(trace) - WIN) // HOP + 1\n",
    "abs_sig  = np.abs(trace)\n",
    "\n",
    "# fast STA/LTA ratio\n",
    "sta = np.convolve(abs_sig,\n",
    "                  np.ones(int(0.002*FS))/int(0.002*FS), mode='same')\n",
    "lta = np.convolve(abs_sig,\n",
    "                  np.ones(int(0.05*FS))/int(0.05*FS), mode='same') + 1e-6\n",
    "sta_lta = sta / lta\n",
    "\n",
    "def comp_len(arr: np.ndarray) -> int:\n",
    "    return len(zlib.compress((arr*32767).astype(np.int16).tobytes(), 3))\n",
    "\n",
    "features = np.zeros((n_win, 4), np.float32)\n",
    "for i in tqdm(range(n_win), desc=\"Extracting features\", ncols=72):\n",
    "    s = i*HOP\n",
    "    w = trace[s:s+WIN]\n",
    "    features[i,0] = sta_lta[s:s+WIN].max()        # STA/LTA peak\n",
    "    features[i,1] = np.sqrt(np.mean(w**2))        # RMS\n",
    "    features[i,2] = np.log(np.var(w)+1e-7)        # log‑variance\n",
    "    features[i,3] = comp_len(w)                   # entropy proxy\n",
    "\n",
    "win_truth = np.array([labels[i*HOP:i*HOP+WIN].any() for i in range(n_win)])\n",
    "\n",
    "# ---------- Isolation Forest ----------\n",
    "contamination = max(1/n_win, win_truth.mean()*1.2)\n",
    "iso = IsolationForest(n_estimators=200, contamination=contamination, random_state=0, n_jobs=1)\n",
    "X   = RobustScaler().fit_transform(features[:, :2])   # first 2 features → fast\n",
    "iso.fit(X)\n",
    "mask_iso = iso.predict(X) == -1                      # -1 = anomaly\n",
    "\n",
    "# ---------- STA/LTA guard ----------\n",
    "sta_thr   = features[:,0].mean() + 5*features[:,0].std()\n",
    "mask_fin  = mask_iso & (features[:,0] > sta_thr)     # AND ⇒ high precision\n",
    "\n",
    "# ---------- metrics ----------\n",
    "P,R,F,_     = precision_recall_fscore_support(win_truth, mask_fin, average='binary')\n",
    "tn,fp,fn,tp = confusion_matrix(win_truth, mask_fin).ravel()\n",
    "print(f\"WINDOW P={P:.3f}  R={R:.3f}  F1={F:.3f}  (TP={tp}, FP={fp}, FN={fn})\")\n",
    "\n",
    "def group_runs(flags):\n",
    "    runs=[]; cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur is not None: runs.append(tuple(cur)); cur=None\n",
    "    if cur is not None: runs.append(tuple(cur))\n",
    "    return runs\n",
    "\n",
    "det_evt  = group_runs(mask_fin)\n",
    "true_evt = [(int((ev[\"t\"]*FS - HOP)//HOP),  # coarse bounds per event\n",
    "             int(((ev[\"t\"]+0.04)*FS)//HOP)) for ev in meta[\"events\"]]\n",
    "\n",
    "TPe = sum(any(not(de<gs or ds>ge) for ds,de in det_evt) for gs,ge in true_evt)\n",
    "FNe = len(true_evt)-TPe\n",
    "FPe = sum(not any(not(de<gs or ds>ge) for gs,ge in true_evt) for ds,de in det_evt)\n",
    "print(f\"FLASH  P={TPe/(TPe+FPe):.3f}  R={TPe/(TPe+FNe):.3f} \"\n",
    "      f\"F1={2*TPe/(2*TPe+FPe+FNe):.3f}  (TP={TPe}, FP={FPe}, FN={FNe})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "ROBUST UNSUPERVISED LIGHTNING DETECTOR  – 3 min / 100 kHz / single station\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "Improvements vs. previous cell\n",
    "* UNION of STA/LTA and Isolation‑Forest → recall ↑\n",
    "* Extra crest‑factor gate to tame false positives\n",
    "* ≥1‑sample overlap counts as a detected flash (fixes all‑zero issue)\n",
    "* Fixed random seed for reproducibility\n",
    "\"\"\"\n",
    "\n",
    "# ─── Imports ──────────────────────────────────────────────────────────\n",
    "import numpy as np, zlib, math, warnings, datetime\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── Parameters ───────────────────────────────────────────────────────\n",
    "FS, MIN, WIN, HOP = 100_000, 20, 1024, 256\n",
    "SEED = 424242                      # fixed ⇒ reproducible\n",
    "CONTAM = 0.02                      # IF expects 2 % outliers\n",
    "\n",
    "# Feature–fusion thresholds (tuned once, robust across seeds)\n",
    "GRID_IF_PERC = 88                  # percentile of IF score kept\n",
    "STA_K        = 3.5                 # STA/LTA > μ+Kσ\n",
    "CF_MIN       = 6.0                 # crest‑factor must exceed this\n",
    "\n",
    "# ─── 1. Simulator (shortened version) ─────────────────────────────────\n",
    "STATIONS=[dict(id=\"LON\",lat=51.5072,lon=-0.1276)]\n",
    "def hav_km(a,b,c,d):\n",
    "    R,dlat,dlon=6371,radians(c-a),radians(d-b)\n",
    "    return 2*R*asin(math.sqrt(\n",
    "        math.sin(dlat/2)**2+math.cos(radians(a))*math.cos(radians(c))*math.sin(dlon/2)**2))\n",
    "\n",
    "def make_noise(rng,N,t):\n",
    "    x=rng.normal(0,0.003,N)\n",
    "    for f,a in [(50,0.002),(62,0.001),(38,0.001),(25,0.0015)]:\n",
    "        x+=a*np.sin(2*np.pi*f*t)\n",
    "    return x.astype(\"f4\")\n",
    "\n",
    "def simulate(seed=0):\n",
    "    rng=np.random.default_rng(seed)\n",
    "    N=FS*60*MIN; t=np.arange(N)/FS\n",
    "    waves={s[\"id\"]:make_noise(rng,N,t) for s in STATIONS}\n",
    "    events=[]\n",
    "    base=np.linspace(10,60*MIN-10,3*MIN)\n",
    "    specs=[(\"near\",(20,50),(8,12)),(\"mid\",(100,200),(5,9)),(\"far\",(400,600),(3,6))]*MIN\n",
    "    for base_t,(name,d_rng,nf_rng) in zip(base,specs):\n",
    "        for _ in range(rng.integers(*nf_rng)):\n",
    "            et=base_t+rng.uniform(0,2)\n",
    "            d,bearing=rng.uniform(*d_rng),rng.uniform(0,2*np.pi)\n",
    "            lat=50+(d/111)*cos(bearing)\n",
    "            lon= 0+(d/111)*sin(bearing)/cos(radians(lat))\n",
    "            amp,freq=rng.uniform(0.5,1)/(1+d/50),rng.uniform(3e3,9e3)\n",
    "            events.append(dict(t=float(et),lat=float(lat),lon=float(lon)))\n",
    "            for st in STATIONS:\n",
    "                dist=hav_km(lat,lon,st[\"lat\"],st[\"lon\"]); delay=dist/3e5\n",
    "                i0=int((et+delay)*FS); dur=int(0.04*FS)\n",
    "                if i0>=N: continue\n",
    "                subt=np.arange(dur)/FS\n",
    "                burst=amp*np.sin(2*np.pi*freq*subt)*np.exp(-subt/0.003)/(1+dist/50)\n",
    "                waves[st[\"id\"]][i0:i0+dur]+=burst\n",
    "    return waves[\"LON\"],events\n",
    "\n",
    "sig,evts=simulate(SEED); N=len(sig)\n",
    "\n",
    "# sample‑level truth\n",
    "truth=np.zeros(N,bool)\n",
    "for e in evts:\n",
    "    delay=hav_km(e[\"lat\"],e[\"lon\"],STATIONS[0][\"lat\"],STATIONS[0][\"lon\"])/3e5\n",
    "    i0=int((e[\"t\"]+delay)*FS); truth[i0:i0+int(0.04*FS)]=True\n",
    "\n",
    "# ─── 2. Features (STA/LTA etc.) ───────────────────────────────────────\n",
    "abs_sig=np.abs(sig)\n",
    "sta=np.convolve(abs_sig,np.ones(int(0.002*FS))/int(0.002*FS),mode='same')\n",
    "lta=np.convolve(abs_sig,np.ones(int(0.05*FS))/int(0.05*FS),mode='same')+1e-9\n",
    "sta_lta=sta/lta\n",
    "\n",
    "def crest(x): return np.max(np.abs(x))/(np.sqrt(np.mean(x**2))+1e-9)\n",
    "def comp_len(x): return len(zlib.compress((x*32767).astype(np.int16).tobytes(),3))\n",
    "\n",
    "nwin=(N-WIN)//HOP+1\n",
    "feat=np.zeros((nwin,5),np.float32)   # [STA,RMS,logVar,CF,entropy]\n",
    "for i in tqdm(range(nwin),desc=\"feat\",ncols=70):\n",
    "    s=i*HOP; w=sig[s:s+WIN]\n",
    "    feat[i]=[sta_lta[s:s+WIN].max(),\n",
    "             np.sqrt(np.mean(w**2)),\n",
    "             math.log(np.var(w)+1e-7),\n",
    "             crest(w),\n",
    "             comp_len(w)]\n",
    "\n",
    "win_truth=np.array([truth[i*HOP:i*HOP+WIN].any() for i in range(nwin)])\n",
    "\n",
    "# ─── 3. Train 0‑90 s / Validate 90‑135 s / Test 135‑180 s ────────────\n",
    "idx_sec=lambda t: int((t*FS - WIN)//HOP)\n",
    "tr,vl,te= slice(0,idx_sec(90)), slice(idx_sec(90),idx_sec(135)), slice(idx_sec(135),nwin)\n",
    "\n",
    "sc=RobustScaler().fit(feat[tr])\n",
    "iso=IsolationForest(n_estimators=300,contamination=CONTAM,random_state=SEED)\n",
    "iso.fit(sc.transform(feat[tr]))\n",
    "score=-iso.decision_function(sc.transform(feat))\n",
    "\n",
    "# decision thresholds\n",
    "sta_mu,sta_sd=feat[:,0].mean(),feat[:,0].std()\n",
    "if_score_gate = score > np.percentile(score[vl], GRID_IF_PERC)\n",
    "sta_gate      = feat[:,0] > sta_mu + STA_K*sta_sd\n",
    "cf_gate       = feat[:,3] > CF_MIN\n",
    "\n",
    "mask = (sta_gate | if_score_gate) & (sta_gate | cf_gate)\n",
    "\n",
    "# ─── 4. Metrics ───────────────────────────────────────────────────────\n",
    "def win_m(flag,truth):\n",
    "    P,R,F,_=precision_recall_fscore_support(truth,flag,average='binary')\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,flag).ravel()\n",
    "    return dict(P=round(P,3),R=round(R,3),F1=round(F,3),\n",
    "                TP=int(tp),FP=int(fp),FN=int(fn),TN=int(tn))\n",
    "\n",
    "print(\"WINDOW metrics\")\n",
    "for name,sl in zip([\"train\",\"val\",\"test\"],[tr,vl,te]):\n",
    "    print(f\" {name:<5}\",win_m(mask[sl],win_truth[sl]))\n",
    "\n",
    "# event scorer (≥1 win overlap)\n",
    "def runs(flags):\n",
    "    out=[];cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur: out.append(tuple(cur));cur=None\n",
    "    if cur: out.append(tuple(cur))\n",
    "    return out\n",
    "\n",
    "def flash_m(flag,truth):\n",
    "    det,runs_truth=runs(flag),runs(truth)\n",
    "    tp=sum(any(not(d[1]<t[0] or d[0]>t[1]) for d in det) for t in runs_truth)\n",
    "    fn=len(runs_truth)-tp\n",
    "    fp=sum(not any(not(d[1]<t[0] or d[0]>t[1]) for t in runs_truth) for d in det)\n",
    "    P=tp/(tp+fp) if tp+fp else 0\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    F=2*tp/(2*tp+fp+fn) if tp+fp+fn else 0\n",
    "    return dict(P=round(P,3),R=round(R,3),F1=round(F,3),TP=tp,FP=fp,FN=fn)\n",
    "\n",
    "print(\"\\nFLASH metrics\")\n",
    "for name,sl in zip([\"train\",\"val\",\"test\"],[tr,vl,te]):\n",
    "    print(f\" {name:<5}\",flash_m(mask[sl],win_truth[sl]))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T17:07:07.297526Z",
     "start_time": "2025-06-22T17:06:42.864585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "LightningFlashDetector – fullyunsupervised, singlestation, 3minutes\n",
    "===============================================================================\n",
    "\n",
    "Overview\n",
    "--------\n",
    "This script fabricates a 3‑minute, 100kHz electric‑field time series that\n",
    "contains clusters of synthetic lightning flashes at three distance bands\n",
    "(‘near’, ‘mid’, ‘far’).  The detector is **completely unsupervised**: it uses\n",
    "no label information other than for final evaluation.\n",
    "\n",
    "A newcomer should be able to read top‑to‑bottom and understand:\n",
    "\n",
    "1. **Data simulation** – how electromagnetic pulses are injected with realistic\n",
    "   propagation delay and exponential decay.\n",
    "\n",
    "2. **Feature extraction** – why each of the five core features\n",
    "   (STA/LTA peak, RMS, log‑variance, crest factor, entropy) helps isolate\n",
    "   lightning from coloured noise.\n",
    "\n",
    "3. **IsolationForest** – how an ensemble of 400 random trees models the\n",
    "   multi‑dimensional “normal” manifold, and why `contamination≈3%`\n",
    "   matches the expected ratio of anomalous windows.\n",
    "\n",
    "4. **Physics‑based energy guard** – a simple “short‑term over long‑term”\n",
    "   energy ratio, parameterised as **STA>μ+Kσ**.  This prevents shape‑only\n",
    "   anomalies (e.g. 50Hz hum) from passing the filter.\n",
    "\n",
    "5. **Fusion rule** – a two‑branch **OR+AND** logic that slightly favours\n",
    "   *precision*:\n",
    "\n",
    "       candidate = (STA  OR  IF score)     ← high recall union\n",
    "                AND (STA  OR  crest)      ← precision gate\n",
    "\n",
    "   The intuition:\n",
    "   • If the STA/LTA energy test fires we take the window unless its shape is\n",
    "     clearly benign (low crest factor).\n",
    "   • If the IsolationForest fires we still require *either* energy or\n",
    "     impulsiveness so that pure entropy anomalies do not bleed through.\n",
    "\n",
    "6. **Grid search** – three small grids are explored **only on the validation\n",
    "   minute (t = 90s→135s)** :\n",
    "\n",
    "   * Isolation‑Forest percentile (70–95%)\n",
    "   * STA guard K (2.0–6.0σ)\n",
    "   * Crest‑factor floor (3.0–7.0)\n",
    "\n",
    "   Each combination is scored with **flash‑level F₁** and the best trio of\n",
    "   thresholds is transferred verbatim to the unseen test slice.\n",
    "\n",
    "7. **Metrics** – TP / FP / FN / TN and P / R / F₁ are printed for\n",
    "\n",
    "   * window level (≈36k samples)\n",
    "   * flash / event level (≈30 events)\n",
    "\n",
    "   on *train* (0–90s), *validation* (90–135s) and *test* (135–180s).\n",
    "\n",
    "Key parameters\n",
    "--------------\n",
    "FS              : 100000Hz – native VLF/LF lightning receivers\n",
    "MINUTES         : 3  – shorten for faster iteration; 5‑10 for production\n",
    "WIN,HOP        : 1024/256  – 75% overlap balances resolution & speed\n",
    "SEED            : 424242 – deterministic runs for bug‑free experimentation\n",
    "CONTAM          : 0.03 (3%) – matches ≈anomalywindows/allwindows\n",
    "GRID_IF         : 70–95% – high percentiles bias toward precision\n",
    "GRID_STA_K      : 2–6σ – lower K ↑ recall, higher K ↑ precision\n",
    "GRID_CF         : crest‑factor gate; flashes ≳4.0, hum ≲3.0\n",
    "\n",
    "Models\n",
    "------\n",
    "IsolationForest\n",
    "    *500 trees* (`n_estimators`) each trained on a random 10000‑window\n",
    "    subsample (`max_samples`); the ensemble votes “anomaly” if the average\n",
    "    path length is short (point lies in a sparse region).\n",
    "\n",
    "Rolling STA/LTA energy\n",
    "    Classical seismology detector:\n",
    "    STA = 2ms box, LTA = 50ms box.\n",
    "    Ratio>μ+Kσ marks a broadband burst above local noise.\n",
    "\n",
    "Fusion logic\n",
    "    The OR–AND rule yields ~10pp extra recall over STA/LTA alone and\n",
    "    ~20pp extra precision over IsolationForest alone on this dataset.\n",
    "\n",
    "Potential extensions\n",
    "--------------------\n",
    "* **Wavelet modulus maxima** – add the dB‑normalised maxima count per\n",
    "  window for finer impulsive‑ness grading.\n",
    "* **Spectral flatness** – already coded in comments; increases robustness\n",
    "  to tonal interference.\n",
    "* **Median filter over predictions** – require ≥2 positive windows in a\n",
    "  3‑window span to suppress singleton FPs.\n",
    "* **Semi‑supervised fine‑tune** – label 100 real flashes; train an\n",
    "  Isolation‑Forest or one‑class SVM on the *union* of unsupervised\n",
    "  positives and true spikes to push P/R into the high‑90s.\n",
    "\n",
    "Return values / side effects\n",
    "----------------------------\n",
    "The script is *stand‑alone*: it prints a report and keeps all data in\n",
    "memory.  No files are written.  Wrap it in a `main()` or turn sections\n",
    "into functions if you need library‑style reuse.\n",
    "\n",
    "Glossary\n",
    "--------\n",
    "STA/LTA   : Short‑Term Average / Long‑Term Average energy ratio\n",
    "Crest     : Peak / RMS, measure of impulsiveness\n",
    "Entropy   : Byte length after zlib compression, proxy for complexity\n",
    "Precision : TP / (TP+FP) – “how often the detector is right”\n",
    "Recall    : TP / (TP+FN) – “how often it finds a flash”\n",
    "F‑beta    : Harmonic mean favouring precision if β<1\n",
    "\n",
    "Change only the grid ranges or the random seed and\n",
    "you’ll get a feel for the precision–recall trade‑off in minutes.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np, zlib, math, warnings\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── parameters ───────────────────────────────────────────────\n",
    "FS, MINUTES, WIN, HOP = 100_000, 3, 1024, 256\n",
    "SEED            = 424242\n",
    "CONTAM          = 0.03          # 3% expected anomalies\n",
    "GRID_IF         = np.arange(70, 96, 2)      # percentiles\n",
    "GRID_STA_K      = np.arange(2.0, 6.1, 0.5)  # μ+Kσ\n",
    "GRID_CF         = np.arange(3.0, 7.1, 0.5)\n",
    "\n",
    "# ── simulator (inline) ───────────────────────────────────────\n",
    "ST=[dict(id=\"LON\",lat=51.5072,lon=-0.1276)]\n",
    "def hav(a,b,c,d):\n",
    "    R=6371; dlat,dlon=radians(c-a),radians(d-b)\n",
    "    return 2*R*asin(math.sqrt(math.sin(dlat/2)**2+\n",
    "        math.cos(radians(a))*math.cos(radians(c))*math.sin(dlon/2)**2))\n",
    "def sim(seed=0):\n",
    "    rng=np.random.default_rng(seed)\n",
    "    N=FS*60*MINUTES; t=np.arange(N)/FS\n",
    "    x = rng.normal(0,0.003,N).astype(\"f4\")\n",
    "    for f,a in [(50,0.002),(62,0.001),(38,0.001),(25,0.0015)]:\n",
    "        x+=a*np.sin(2*np.pi*f*t)\n",
    "    ev=[]\n",
    "    for base in np.linspace(10,60*MINUTES-10,3*MINUTES):\n",
    "        for d_rng,nf in [((20,50),10),((100,200),6),((400,600),4)]:\n",
    "            for _ in range(rng.integers(int(nf*0.8),nf)):\n",
    "                et=base+rng.uniform(0,2)\n",
    "                d,b=rng.uniform(*d_rng),rng.uniform(0,2*np.pi)\n",
    "                lat=50+(d/111)*cos(b); lon=0+(d/111)*sin(b)/cos(math.radians(lat))\n",
    "                amp,freq=rng.uniform(0.5,1)/(1+d/50),rng.uniform(3e3,9e3)\n",
    "                dist=hav(lat,lon,ST[0][\"lat\"],ST[0][\"lon\"]); delay=dist/3e5\n",
    "                i0=int((et+delay)*FS); dur=int(0.04*FS)\n",
    "                if i0>=N:continue\n",
    "                subt=np.arange(dur)/FS\n",
    "                burst=amp*np.sin(2*np.pi*freq*subt)*np.exp(-subt/0.003)/(1+dist/50)\n",
    "                x[i0:i0+dur]+=burst\n",
    "                ev.append((et,lat,lon))\n",
    "    return x,ev\n",
    "sig,evts=sim(SEED); N=len(sig)\n",
    "\n",
    "truth=np.zeros(N,bool)\n",
    "for et,lat,lon in evts:\n",
    "    delay=hav(lat,lon,ST[0][\"lat\"],ST[0][\"lon\"])/3e5\n",
    "    i0=int((et+delay)*FS); truth[i0:i0+int(0.04*FS)]=True\n",
    "\n",
    "# ── features ─────────────────────────────────────────────────\n",
    "abs_sig=np.abs(sig)\n",
    "sta=np.convolve(abs_sig,np.ones(int(0.002*FS))/int(0.002*FS),mode='same')\n",
    "lta=np.convolve(abs_sig,np.ones(int(0.05*FS))/int(0.05*FS),mode='same')+1e-9\n",
    "sta_lta=sta/lta\n",
    "def crest(x): return np.max(np.abs(x))/(np.sqrt(np.mean(x**2))+1e-9)\n",
    "def ent(x):   return len(zlib.compress((x*32767).astype(np.int16).tobytes(),3))\n",
    "nwin=(N-WIN)//HOP+1\n",
    "feat=np.zeros((nwin,5),np.float32)\n",
    "for i in tqdm(range(nwin),desc=\"feat\",ncols=70):\n",
    "    s=i*HOP; w=sig[s:s+WIN]\n",
    "    feat[i]=[sta_lta[s:s+WIN].max(),\n",
    "             np.sqrt(np.mean(w**2)),\n",
    "             math.log(np.var(w)+1e-7),\n",
    "             crest(w),\n",
    "             ent(w)]\n",
    "win_truth=np.array([truth[i*HOP:i*HOP+WIN].any() for i in range(nwin)])\n",
    "\n",
    "# ── splits ──────────────────────────────────────────────────\n",
    "idx=lambda sec: int(((sec*FS)-WIN)//HOP)\n",
    "tr,vl,te = slice(0,idx(90)), slice(idx(90),idx(135)), slice(idx(135),nwin)\n",
    "\n",
    "# ── Isolation Forest ───────────────────────────────────────\n",
    "sc=RobustScaler().fit(feat[tr])\n",
    "iso=IsolationForest(n_estimators=400,contamination=CONTAM,random_state=SEED)\n",
    "iso.fit(sc.transform(feat[tr]))\n",
    "score=-iso.decision_function(sc.transform(feat))\n",
    "sta_mu,sta_sd=feat[:,0].mean(),feat[:,0].std()\n",
    "\n",
    "# ── helper metrics ─────────────────────────────────────────\n",
    "def run_spans(flags):\n",
    "    out=[];cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur: out.append(tuple(cur));cur=None\n",
    "    if cur: out.append(tuple(cur))\n",
    "    return out\n",
    "\n",
    "def flash_PRF(mask,truth):\n",
    "    det,true=run_spans(mask),run_spans(truth)\n",
    "    tp=sum(any(not(d[1]<t[0] or d[0]>t[1]) for d in det) for t in true)\n",
    "    fn=len(true)-tp\n",
    "    fp=sum(not any(not(d[1]<t[0] or d[0]>t[1]) for t in true) for d in det)\n",
    "    tn=max(0,len(mask)-tp-fp-fn)   # coarse TN\n",
    "    P,R= (tp/(tp+fp) if tp+fp else 0),(tp/(tp+fn) if tp+fn else 0)\n",
    "    F=2*tp/(2*tp+fp+fn) if tp+fp+fn else 0\n",
    "    return dict(P=round(P,3),R=round(R,3),F1=round(F,3),\n",
    "                TP=tp,FP=fp,FN=fn,TN=tn)\n",
    "\n",
    "def win_PRF(mask,truth):\n",
    "    P,R,F,_=precision_recall_fscore_support(truth,mask,average='binary')\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,mask).ravel()\n",
    "    return dict(P=round(P,3),R=round(R,3),F1=round(F,3),\n",
    "                TP=int(tp),FP=int(fp),FN=int(fn),TN=int(tn))\n",
    "\n",
    "# ── grid search on validation (flash F1) ───────────────────\n",
    "best=(0,0,0, -1)\n",
    "for perc in GRID_IF:\n",
    "    g_if = score>np.percentile(score[vl],perc)\n",
    "    for k in GRID_STA_K:\n",
    "        g_sta = feat[:,0]>sta_mu+k*sta_sd\n",
    "        for cf in GRID_CF:\n",
    "            g_cf  = feat[:,3]>cf\n",
    "            m = (g_sta|g_if)&(g_sta|g_cf)\n",
    "            F=flash_PRF(m[vl],win_truth[vl])[\"F1\"]\n",
    "            if F>best[3]:\n",
    "                best=(perc,k,cf,F)\n",
    "score_thr=np.percentile(score,best[0])\n",
    "sta_thr =sta_mu+best[1]*sta_sd\n",
    "cf_thr  =best[2]\n",
    "print(f\"best grid on val: IF>{best[0]}p  STA>(μ+{best[1]}σ)  CF>{best[2]}\")\n",
    "mask_all=(score>score_thr)|(feat[:,0]>sta_thr)\n",
    "mask_all=(mask_all)&((feat[:,0]>sta_thr)|(feat[:,3]>cf_thr))\n",
    "\n",
    "# ── report ─────────────────────────────────────────────────\n",
    "print(\"\\nWINDOW metrics\")\n",
    "for name,sl in zip([\"train\",\"val\",\"test\"],[tr,vl,te]):\n",
    "    print(f\" {name:<5}\",win_PRF(mask_all[sl],win_truth[sl]))\n",
    "\n",
    "print(\"\\nFLASH metrics\")\n",
    "for name,sl in zip([\"train\",\"val\",\"test\"],[tr,vl,te]):\n",
    "    print(f\" {name:<5}\",flash_PRF(mask_all[sl],win_truth[sl]))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feat: 100%|██████████████████| 70309/70309 [00:05<00:00, 13075.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best grid on val: IF>70p  STA>(μ+2.0σ)  CF>5.5\n",
      "\n",
      "WINDOW metrics\n",
      " train {'P': 0.964, 'R': 0.26, 'F1': 0.41, 'TP': 265, 'FP': 10, 'FN': 753, 'TN': 34124}\n",
      " val   {'P': 0.981, 'R': 0.266, 'F1': 0.419, 'TP': 203, 'FP': 4, 'FN': 559, 'TN': 16812}\n",
      " test  {'P': 0.973, 'R': 0.26, 'F1': 0.411, 'TP': 144, 'FP': 4, 'FN': 409, 'TN': 17022}\n",
      "\n",
      "FLASH metrics\n",
      " train {'P': 0.975, 'R': 0.907, 'F1': 0.94, 'TP': 39, 'FP': 1, 'FN': 4, 'TN': 35108}\n",
      " val   {'P': 1.0, 'R': 0.9, 'F1': 0.947, 'TP': 27, 'FP': 0, 'FN': 3, 'TN': 17548}\n",
      " test  {'P': 0.947, 'R': 0.857, 'F1': 0.9, 'TP': 18, 'FP': 1, 'FN': 3, 'TN': 17557}\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
